# 小红书个人账户笔记爬取项目 📄

本项目旨在对小红书网页个人账户的发帖内容进行爬取，并将结果生成 Excel 文件。通过简单的配置，您可以快速抓取指定用户的发帖数据，保存并用于分析。

## 安装 🛠️

在开始使用该项目之前，请确保已经安装了所需的依赖库。您可以通过以下命令进行安装：

```bash
pip install -r requirements.txt
```

## 配置 ⚙️

### 1. 配置 Cookies

为确保爬虫能够正常访问小红书网页，您需要在 `xhs.py` 和 `test.py` 中配置有效的 Cookies。具体步骤如下：

- 打开 `xhs.py` 文件。
- 找到相应位置，将您的小红书 Cookies 粘贴到代码中。

### 2. 配置需要爬取的用户 ID

在 `xhs.py` 文件中，您可以设置需要爬取的目标用户 ID。找到相应的变量或配置项，输入您想要爬取的用户 ID。

### 3. 搜索示例配置

若您希望通过关键词搜索进行数据抓取，可以打开 `test.py` 文件并配置相应的 Cookies，操作方式与上文相同。

## 运行 🚀

配置完成后，您可以通过以下命令运行爬虫程序：

- 运行 `xhs.py` 进行指定用户发帖数据的爬取：

```bash
python xhs.py
```

- 运行 `test.py` 进行关键词搜索示例：

```bash
python test.py
```

## 注意事项 ⚠️

- 确保您已配置正确的 Cookies 和用户 ID。
- 爬取小红书数据时，请遵循相关法律法规和平台规定，避免违反用户隐私或平台政策。

## 常见问题解答（FAQ） 💬

### 1. 为什么代码是同步的，没有使用线程池等操作？ 🕒

我个人目前并不需要这么高并发的爬取需求，因此选择了同步处理方式。后续可能会根据项目需求进行更新迭代，加入更多并发或异步处理的优化。

### 2. 为什么是补环境而不是纯算法？ 🤖

我倾向于选择最简单的逻辑路线——哪个方法简单就走哪个方法，这样可以留出更多的时间去享受生活。最终目标是完成任务，并在过程中保持轻松愉快的体验。

### 3. 为什么代码质量不高？ 🤷‍♂️

代码质量是一个见仁见智的问题。对于我个人而言，代码写得顺手、写得爽就足够了。当然，未来可能会根据需要进行重构和优化，但目前这已经满足我的需求。

### 4. 补环境的 XS 爬虫只能用于个人主页和搜索吗？ 🔍

不一定。通过修改 `c` 值和 `i` 值，您可以灵活地爬取其他数据。具体的修改方式视您的需求而定，该爬虫具有一定的可扩展性。

---

如果您有更多问题，欢迎反馈。😊
